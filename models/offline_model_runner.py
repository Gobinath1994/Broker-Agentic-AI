import requests  # For making HTTP POST requests to the local LLM API

def run_llm(prompt, max_tokens=512):
    """
    Sends a prompt to a locally hosted LLM API and retrieves the generated completion.

    Args:
        prompt (str): The instruction or context you want the language model to respond to.
        max_tokens (int): The maximum number of tokens (words/pieces) in the model's output.

    Returns:
        str: The text response generated by the model, stripped of leading/trailing whitespace.
    """

    # Prepare the payload for the POST request
    payload = {
        "prompt": prompt,            # Input prompt to guide the model's response
        "max_tokens": max_tokens,    # Limit the length of the model's response
        "temperature": 0.7,          # Controls randomness (0 = deterministic, 1 = more creative)
        "stop": ["\n"]               # Optional stopping sequence (stop when newline is generated)
    }

    # Send a POST request to the local LLM server at port 1234
    response = requests.post("http://192.168.0.14:1234/v1/completions", json=payload)

    # Extract and return the model's text output from the JSON response
    return response.json()['choices'][0]['text'].strip()